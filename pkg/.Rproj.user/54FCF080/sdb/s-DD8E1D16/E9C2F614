{
    "collab_server" : "",
    "contents" : "\n######################\n## get.correlations ##\n######################\n\n########################################################################\n\n###################\n## DOCUMENTATION ##\n###################\n\n#' Get significant SNPs, according to a given test of association.\n#'\n#' Identify which SNPs are deemed to be significantly associated with a phenotype,\n#' according to a given test of association and p-value.\n#' (Serves as the treeWAS association testing function;\n#' runs the \\code{assoc.test} function internally.)\n#'\n#' @param snps A matrix containing the real snps.\n#' @param snps.sim A matrix or list of matrices containing simulated snps.\n#' @param phen A factor or vector containing the phenotype (only allowed to contain two levels for now).\n#' @param tree A phylo object containing a phylogenetic tree in which the number of tips is equal to the\n#' length of \\code{phen} and the number of rows of \\code{snps} and \\code{snps.sim}.\n#' @param test A character string or vector containing one or more of the following available tests of association:\n#' \"terminal\", \"simultaneous\", \"subsequent\", \"cor\", \"fisher\". By default, the terminal test is run\n#' (note that within treeWAS, the first three tests are run in a loop by default).\n#' See details for more information on what these tests do and when they may be appropriate.\n#' @param n.tests An integer between 1 and 5 specifying the number of tests you are running on all loci,\n#' to be used in appropriately correcting for multiple testing.\n#' (i.e., the number of times you will be running the \\code{get.sig.snps} function).\n#' @param p.value A single number specifying the p.value below which correlations are deemed to be 'significant'.\n#' @param p.value.correct Specify if/how to correct for multiple testing:\n#' either FALSE, or one of 'bonf' or 'fdr' (indicating, respectively,\n#' the Bonferroni and False Discovery Rate corrections). By default, 'bonf' is selected\n#' @param p.value.by Specify how to determine the location of the p.value threshold:\n#' either 'count' or 'density' (indicating, respectively, that the p.value threshold should\n#' be determined by exact count or with the use of a density function).\n#'\n#'\n#' @author Caitlin Collins \\email{caitiecollins@@gmail.com}\n#' @examples\n#'\n#' ## load data\n#' data(dist)\n#' str(dist)\n#'\n#' ## basic use of fn\n#'\n#' fn(arg1, arg2)\n#'\n#' #' ## more elaborate use of fn\n#' fn(arg1, arg2)\n#'\n#' @export\n\n\n##########\n## FDR: ##\n##########\n\n## NOTES: ##\n## FDR works by managing the number of FALSE discoveries, RELATIVE to the number of TOTAL discoveries.\n## Maintains Q = FALSE discoveries / TOTAL discoveries.\n## Eg. For Q = 0.05, both 5/100 and 50/1000 meet the criterion.\n## Hence, FDR is described as being both adaptive and scalable.\n\n## QUESTION! ##\n## HOW SHOULD WE HANDLE MULTIPLE TESTING CORRECTION WITH FDR WHEN RUNNING MULITPLE TESTS OF ASSOC??????\n## Eg. Bonf --> multiply divisor by 3\n## BUT--if we just run FDR p-value correction by multiplying the \"n.tests\" by n.tests,\n## would the result not be that we just increase the number of false positives accepted in each test?!\n## Could we pool the test results somehow??\n## Or is it OK to ignore the performance of multiple separate assoc tests when using FDR??\n\n########################################################################\n\nget.sig.snps <- function(snps,\n                         snps.unique = NULL,\n                         snps.index = NULL,\n                         snps.sim,\n                         snps.sim.unique = NULL,\n                         snps.sim.index = NULL,\n                         phen,\n                         tree,\n                         test = \"terminal\",\n                         n.tests = 1,\n                         p.value = 0.001,\n                         p.value.correct = \"bonf\",\n                         p.value.by = \"count\",\n                         snps.reconstruction,\n                         snps.sim.reconstruction,\n                         phen.reconstruction){\n\n  #################\n  ## HANDLE SNPS ##\n  #################\n\n  ##########\n  ## snps ##\n  ##########\n\n  if(is.null(snps.unique)){\n    ## Check snps column names\n    if(is.null(colnames(snps))) colnames(snps) <- c(1:ncol(snps))\n\n    ## Get UNIQUE snps + index\n    snps.ori <- snps\n    temp <- get.unique.matrix(snps, MARGIN=2)\n    snps.unique <- temp$unique.data\n    snps.index <- temp$index\n    snps <- snps.unique\n\n    ## record whether all snps are unique or not for later:\n    if(ncol(snps.unique) == ncol(snps.ori)){\n      all.unique <- TRUE\n    }else{\n      all.unique <- FALSE\n    }\n  }else{\n    ## If snps.unique provided as well as snps:\n\n    ## CHECK: is index provided as well??\n    if(is.null(snps.index)){\n      warning(\"if snps.unique is provided,\n              snps.index must also be provided to indicate\n              original mapping locations for all unique sites.\n              Ignoring unique snps provided; working with snps only.\")\n\n      ## repeat above steps (as if no snps.unique was provided):\n      ## Check snps column names\n      if(is.null(colnames(snps))) colnames(snps) <- c(1:ncol(snps))\n\n      ## Get UNIQUE snps + index\n      snps.ori <- snps\n      temp <- get.unique.matrix(snps, MARGIN=2)\n      snps.unique <- temp$unique.data\n      snps.index <- temp$index\n      snps <- snps.unique\n\n      ## record whether all snps are unique or not for later:\n      if(ncol(snps.unique) == ncol(snps.ori)){\n        all.unique <- TRUE\n      }else{\n        all.unique <- FALSE\n      }\n\n    }else{\n\n      snps.ori <- snps\n      snps <- snps.unique\n\n      ## record whether all snps are unique or not for later:\n      if(ncol(snps.unique) == ncol(snps.ori)){\n        all.unique <- TRUE\n      }else{\n        all.unique <- FALSE\n      }\n    }\n  }\n\n  ##############\n  ## snps.sim ##\n  ##############\n  ## Handle matrix/list input:\n  if(class(snps.sim) == \"list\"){\n    ## If list of length 1...\n    if(length(snps.sim) == 1){\n      ## keep matrix:\n      snps.sim <- snps.sim[[1]]\n    }else{\n      ## If list of multiple matrices...\n      ## merge all elements into one big matrix\n      ## by pasting columns together:\n      snps.sim <- do.call(\"cbind\", snps.sim)\n    }\n  }\n\n  ## Get UNIQUE snps.sim + index\n  if(is.null(snps.sim.unique)){\n    snps.sim.ori <- snps.sim\n    temp <- get.unique.matrix(snps.sim, MARGIN=2)\n    snps.sim.unique <- temp$unique.data\n    snps.sim.index <- temp$index\n    snps.sim <- snps.sim.unique\n\n    ## record whether all snps are unique or not for later:\n    if(ncol(snps.sim.unique) == ncol(snps.sim.ori)){\n      all.unique.sim <- TRUE\n    }else{\n      all.unique.sim <- FALSE\n    }\n  }else{\n    ## If snps.sim.unique provided as well as snps:\n\n    ## CHECK: is index provided as well??\n    if(is.null(snps.sim.index)){\n      warning(\"if snps.sim.unique is provided,\n              snps.sim.index must also be provided to indicate\n              original mapping locations for all unique sites.\n              Ignoring unique snps.sim provided; working with snps.sim only.\")\n\n      ## repeat above steps (as if no snps.unique was provided):\n      snps.sim.ori <- snps.sim\n      temp <- get.unique.matrix(snps.sim, MARGIN=2)\n      snps.sim.unique <- temp$unique.data\n      snps.sim.index <- temp$index\n      snps.sim <- snps.sim.unique\n\n      ## record whether all snps are unique or not for later:\n      if(ncol(snps.sim.unique) == ncol(snps.sim.ori)){\n        all.unique.sim <- TRUE\n      }else{\n        all.unique.sim <- FALSE\n      }\n    }else{\n\n      snps.sim.ori <- snps.sim\n      snps.sim <- snps.sim.unique\n\n      ## record whether all snps are unique or not for later:\n      if(ncol(snps.sim.unique) == ncol(snps.sim.ori)){\n        all.unique.sim <- TRUE\n      }else{\n        all.unique.sim <- FALSE\n      }\n    }\n  }\n\n  #################\n  ## HANDLE PHEN ##\n  #################\n  ## convert phenotype to numeric:\n  phen.ori <- phen\n  if(!is.numeric(phen)) phen <- as.numeric(phen)\n  ## for ease of interpretation,\n  ## if phen has 2 levels, 1 and 2,\n  ## make these 0 and 1:\n  if(length(unique(phen))!=2){\n    stop(\"This function is only designed for phenotypes with two levels.\")\n  }else{\n    if(length(phen[-c(which(phen==1), which(phen==2))])==0){\n      phen <- replace(phen, which(phen==1), 0)\n      phen <- replace(phen, which(phen==2), 1)\n    }\n  }\n  ## ensure ind names not lost\n  names(phen) <- names(phen.ori)\n\n\n  ######################\n  ## ASSOCIATION TEST ##\n  ######################\n\n  if(test != \"simultaneous\" & test != \"subsequent\"){\n\n    ########################################################\n    ## Calculate correlations btw REAL SNPs and phenotype ##\n    ########################################################\n    corr.dat <- assoc.test(snps=snps, phen=phen, tree=NULL, test=test)\n\n\n    #############################################################\n    ## Calculate correlations btw SIMULATED SNPs and phenotype ##\n    #############################################################\n    corr.sim <- assoc.test(snps=snps.sim, phen=phen, tree=NULL, test=test)\n\n  }else{\n\n    #####################################\n    ## SIMULTANEOUS & SUBSEQUENT TESTS ##\n    #####################################\n    ## (run w/ RECONSTRUCTIONS) ##\n\n    ############################\n    ## HANDLE RECONSTRUCTIONS ##\n    ############################\n\n    ## NOTE: If snps(.sim) are UNIQUE but snps(.sim).reconstruction are NOT, test will give INCORRECT OUTPUT!!!\n\n    if(all.unique == FALSE){\n      ## check if snsp.rec is already in UNIQUE form:\n      if(ncol(snps.reconstruction) != ncol(snps.unique)){\n        temp <- get.unique.matrix(snps.reconstruction, MARGIN=2)\n        snps.reconstruction <- temp$unique.data\n        snps.reconstruction.index <- temp$index\n        if(!identical(snps.reconstruction.index, snps.index)){\n          warning(\"Careful-- snps and snps.reconstruction should have the same index when reduced\n                  to their unique forms!\") ## SHOULD THIS BE A \"STOP\" INSTEAD? OR IS THIS ERROR NOT FATAL OR NOT POSSIBLE????\n        }\n      }else{\n        snps.reconstruction.index <- snps.index\n      }\n    }\n\n    if(all.unique.sim == FALSE){\n      ## check if snsp.rec is already in UNIQUE form:\n      if(ncol(snps.reconstruction) != ncol(snps.unique)){\n        temp <- get.unique.matrix(snps.sim.reconstruction, MARGIN=2)\n        snps.sim.reconstruction <- temp$unique.data\n        snps.sim.reconstruction.index <- temp$index\n        if(!identical(snps.sim.reconstruction.index, snps.sim.index)){\n          warning(\"Careful-- snps.sim and snps.sim.reconstruction should have the same index when reduced\n                  to their unique forms!\") ## SHOULD THIS BE A \"STOP\" INSTEAD? OR IS THIS ERROR NOT FATAL OR NOT POSSIBLE????\n        }\n      }else{\n        snps.sim.reconstruction.index <- snps.sim.index\n      }\n    }\n\n    ########################################################\n    ## Calculate correlations btw REAL SNPs and phenotype ##\n    ########################################################\n    ## NEW TEMPORARY CODE to save pagel-like score3 alternatives & raw data:\n    if(test == \"subsequent\"){\n      corr.dat.list <- assoc.test(snps=snps.reconstruction, phen=phen.reconstruction, tree=tree, test=test)\n    }else{\n      corr.dat <- assoc.test(snps=snps.reconstruction, phen=phen.reconstruction, tree=tree, test=test)\n    }\n\n\n    #############################################################\n    ## Calculate correlations btw SIMULATED SNPs and phenotype ##\n    #############################################################\n    ## NEW TEMPORARY CODE to save pagel-like score3 alternatives & raw data:\n    if(test == \"subsequent\"){\n      corr.sim.list <- assoc.test(snps=snps.sim.reconstruction, phen=phen.reconstruction, tree=tree, test=test)\n    }else{\n      corr.sim <- assoc.test(snps=snps.sim.reconstruction, phen=phen.reconstruction, tree=tree, test=test)\n    }\n  }\n\n  ###################################\n  ## HANDLE DUPLICATE SNPS COLUMNS ##\n  ###################################\n\n  if(test != \"subsequent\"){\n  ## Expand corr.dat (if not all snps columns unique):\n  if(all.unique == FALSE){\n    corr.dat.complete <- corr.dat[snps.index]\n    names(corr.dat.complete) <- colnames(snps.ori)\n    corr.dat <- corr.dat.complete\n  }\n\n  ## Expand corr.sim (if not all snps.sim columns unique):\n  if(all.unique.sim == FALSE){\n    corr.sim.complete <- corr.sim[snps.sim.index]\n    names(corr.sim.complete) <- colnames(snps.sim.ori)\n    corr.sim <- corr.sim.complete\n  }\n  }else{\n\n\n    ## NEW TEMPORARY CODE TO SAVE SUBSEQUENT SCORE ALTERNATIVES/RAW DATA:\n    cd.list <- list()\n    for(n in 1:length(corr.dat.list[[2]])){ # 1:8\n      corr.dat <- corr.dat.list[[2]][[n]]\n      ## Expand corr.dat (if not all snps columns unique):\n      if(all.unique == FALSE){\n        corr.dat.complete <- corr.dat[snps.index]\n        names(corr.dat.complete) <- colnames(snps.ori)\n        corr.dat <- corr.dat.complete\n      }\n      cd.list[[n]] <- corr.dat\n    }\n    names(cd.list) <- names(corr.dat.list)\n\n    cs.list <- list()\n    for(n in 1:length(corr.sim.list[[2]])){\n      corr.sim <- corr.sim.list[[2]][[n]]\n      ## Expand corr.sim (if not all snps.sim columns unique):\n      if(all.unique.sim == FALSE){\n        corr.sim.complete <- corr.sim[snps.sim.index]\n        names(corr.sim.complete) <- colnames(snps.sim.ori)\n        corr.sim <- corr.sim.complete\n      }\n      cs.list[[n]] <- corr.sim\n    }\n    names(cs.list) <- names(corr.sim.list)\n\n    SCORE3 <-list(corr.dat = cd.list,\n                  corr.sim = cs.list)\n\n    ## save (w random number for now --> use time stamp to differentiate... )\n    # save(SCORE3, file=paste(\"~/treeWAS/misc/SCORE3\", \"set1_31\", \"Rdata\", sep=\".\"))\n    # save(SCORE3, file=paste(\"~/treeWAS/misc/SCORE3\", sample(c(1000:10000), 1), \"Rdata\", sep=\".\"))\n\n\n    ## Then run regularly as before to continue:\n\n    ## Expand corr.dat (if not all snps columns unique):\n    corr.dat <- corr.dat.list[[1]]\n    if(all.unique == FALSE){\n      corr.dat.complete <- corr.dat[snps.index]\n      names(corr.dat.complete) <- colnames(snps.ori)\n      corr.dat <- corr.dat.complete\n    }\n\n    ## Expand corr.sim (if not all snps.sim columns unique):\n    corr.sim <- corr.sim.list[[1]]\n    if(all.unique.sim == FALSE){\n      corr.sim.complete <- corr.sim[snps.sim.index]\n      names(corr.sim.complete) <- colnames(snps.sim.ori)\n      corr.sim <- corr.sim.complete\n    }\n\n  } # end subsequent test (temp)..\n\n  ## quick look at corr.sim & corr.dat\n  # hist(corr.sim, xlim=c(0,1))\n  # hist(corr.dat, xlim=c(0,1))\n\n\n  ############################\n  ## HANDLE P.VALUE OPTIONS ##\n  ############################\n\n  out <- nom <- list()\n  corr.dat.ori <- corr.dat\n  corr.sim.ori <- corr.sim\n\n  for(i in 1:32){\n\n    ## set options for this round: ##\n\n    ## p.value ##\n    if(i %in% 1:8) p.value <- 0.05\n    if(i %in% 9:16) p.value <- 0.01\n    if(i %in% 17:24) p.value <- 0.001\n    if(i %in% 25:32) p.value <- 0.0001\n\n    ## p.value.correct ##\n    if(i %in% c(1:4, 9:12, 17:20, 25:28)){\n      p.value.correct <- \"bonf\"\n    }else{\n      p.value.correct <- \"fdr\"\n    }\n\n    ## p.value.by ##\n    if(i %in% c(1,2,9,10,17,18,25,26, ## bonf\n                5,6,13,14,21,22,29,30)){ ## fdr\n      p.value.by <- \"count\"\n    }else{\n      p.value.by <- \"density\"\n    }\n\n    ## n.snps.sim ##\n    if(i %in% seq(1, 32, 2)){\n      corr.sim <- corr.sim.ori\n    }else{\n      corr.sim <- corr.sim.ori[1:length(corr.dat)]\n    }\n\n    ## generate list of names containing call info to assign to out at end:\n    if(p.value != 0.0001){\n    nom[[i]] <- paste(\"pval\", p.value,\n                      p.value.correct,\n                      p.value.by,\n                      length(corr.sim)/length(corr.dat), \"x.n.snps\",\n                      sep=\".\")\n    }else{\n      nom[[i]] <- paste(\"pval\", \"0.0001\",\n                        p.value.correct,\n                        p.value.by,\n                        length(corr.sim)/length(corr.dat), \"x.n.snps\",\n                        sep=\".\")\n    }\n\n    #####################\n    ## p.value.correct ##\n    #####################\n    if(p.value.correct == \"bonf\"){\n      ##########\n      ## bonf ##\n      ##########\n\n      p.value <- p.value/(length(corr.dat)*n.tests)\n\n      ################\n      ## p.value.by ##\n      ################\n      if(p.value.by == \"count\") thresh <- quantile(corr.sim, probs=1-p.value)\n      if(p.value.by == \"density\") thresh <- quantile(density(corr.sim)$x, probs=1-p.value)\n    }\n\n\n    if(p.value.correct == \"fdr\"){\n      #########\n      ## fdr ##\n      #########\n      p.vals <- .get.p.vals(corr.sim,\n                            corr.dat = NULL,\n                            fisher.test = TRUE)\n      p.vals <- sort(p.vals, decreasing=TRUE)\n      p.fdr <- p.adjust(p.vals, method=\"fdr\", n=length(p.vals)*n.tests) # CHECK--IS THIS THE CORRECT MT APPROACH FOR FDR????????????????????????\n      p.thresh <- quantile(p.fdr, probs=1-p.value)\n\n      ################\n      ## p.value.by ##\n      ################\n      if(p.value.by == \"count\") thresh <- quantile(corr.sim, probs=p.thresh)\n      if(p.value.by == \"density\") thresh <- quantile(density(corr.sim)$x, probs=p.thresh)\n\n    } # end p.value.correct == \"fdr\"\n\n\n    ##################################\n    ## GET SIGNIFICANT CORRELATIONS ##\n    ##################################\n    if(test==\"fisher\"){\n      ## Identify (real) SNPs w correlations > thresh:\n      sig.snps <- which(corr.dat < thresh)\n      p.vals <- .get.p.vals(corr.sim = corr.sim,\n                            corr.dat = corr.dat,\n                            fisher.test = TRUE)\n      sig.p.vals <- p.vals[sig.snps]\n    }else{\n      ## Identify (real) SNPs w correlations > thresh:\n      sig.snps <- which(corr.dat > thresh)\n      p.vals <- .get.p.vals(corr.sim = corr.sim,\n                            corr.dat = corr.dat,\n                            fisher.test = FALSE)\n      sig.p.vals <- p.vals[sig.snps]\n    }\n\n\n\n    ## 0 p.vals\n    min.p <- paste(\"p-values listed as 0 are <\",\n                   1/length(corr.sim), sep=\" \") ## CHECK---IS THIS RIGHT? SHOULD WE BE MULTIPLYING THE DIVISOR BY N.TESTS ??????????\n\n    ## get list of those correlation values\n    sig.corrs <- corr.dat[sig.snps]\n    ## get the list of those SNPs (ie. their locus names)\n    # sig.snps <- dimnames(snps)[[2]][sig.snps]\n    sig.snps.names <- dimnames(snps.ori)[[2]][sig.snps]\n\n    ## re-order list of sig.snps and sig.corrs by value of sig.corr\n    if(test==\"fisher\"){\n      NWO <- order(sig.corrs, decreasing=FALSE)\n    }else{\n      NWO <- order(sig.corrs, decreasing=TRUE)\n    }\n    sig.snps <- sig.snps[NWO]\n    sig.corrs <- sig.corrs[NWO]\n    sig.p.vals <- sig.p.vals[NWO]\n    sig.snps.names <- sig.snps.names[NWO]\n    gc()\n\n    #################\n    ## GET RESULTS ##\n    #################\n\n    out[[i]] <- list(corr.dat,\n                corr.sim,\n                p.vals,\n                thresh,\n                sig.snps.names,\n                sig.snps,\n                sig.corrs,\n                sig.p.vals,\n                min.p)\n\n    names(out[[i]]) <- c(\"corr.dat\",\n                        \"corr.sim\",\n                        \"p.vals\",\n                        \"sig.thresh\",\n                        \"sig.snps.names\",\n                        \"sig.snps\",\n                        \"sig.corrs\",\n                        \"sig.p.vals\",\n                        \"min.p\")\n\n  } # end for loop\n\n  ## assign names to out containing call info:\n  names(out) <- nom\n\n  if(test == \"subsequent\"){\n  out <- list(\"res\" = out,\n              \"SCORE3\" = SCORE3)\n  }\n\n  return(out)\n\n} # end get.sig.snps\n\n\n\n\n\n\n\n################\n## assoc.test ##\n################\n\n########################################################################\n\n###################\n## DOCUMENTATION ##\n###################\n\n#' Run a test of association between SNPs and a phenotype.\n#'\n#' Run one of five tests of association between each column of a SNPs matrix and a phenotype\n#' (some tests only implemented for \\emph{binary} SNPs and phenotype).\n#'\n#' @param snps A matrix containing the real snps.\n#' @param phen A factor or vector containing the phenotype (only allowed to contain two levels for now).\n#' @param test A character string or vector containing one or more of the following available tests of association:\n#' \"terminal\", \"simultaneous\", \"subsequent\", \"cor\", \"fisher\". By default, the first three tests are run.\n#' See details for more information on what these tests do and when they may be appropriate.\n#'\n#'\n#' @author Caitlin Collins \\email{caitiecollins@@gmail.com}\n#'\n#' @export\n\n########################################################################\n\nassoc.test <- function(snps,\n                       phen,\n                       tree = NULL,\n                       test = c(\"terminal\",\n                                \"simultaneous\",\n                                \"subsequent\",\n                                \"cor\",\n                                \"fisher\")){\n\n  ##########################################\n  ## TERMINAL (test 1: correlation score) ##\n  ##########################################\n  if(test==\"terminal\"){\n    # ~ Correlation \"SCORE\" =\n    # ((nS1P1 + nS0P0) - (nS1P0 + nS0P1) / (n.total))\n    ## must be calculated for each SNP individually...\n    corr.dat <- sapply(c(1:ncol(snps)), function(e)\n      (((length(which(snps[which(phen==1),e]==1)) +\n           length(which(snps[which(phen==0),e]==0)))\n        - (length(which(snps[which(phen==1),e]==0)) +\n             length(which(snps[which(phen==0),e]==1))))\n       / nrow(snps)))\n  } # end test terminal\n\n  #################\n  ## CORRELATION ##\n  #################\n  if(test==\"cor\"){\n    corr.dat <- sapply(c(1:ncol(snps)), function(e)\n      cor(snps[,e], phen)) # regular correlation...\n  } # end test cor\n\n  #########################\n  ## FISHER'S EXACT TEST ##\n  #########################\n  if(test==\"fisher\"){\n    corr.dat <- sapply(c(1:ncol(snps)),\n                       function(e) fisher.test(snps[,e],\n                                               y=phen, alternative=\"two.sided\")$p.value)\n    ## two.sided bc we want to know if inds w\n    ## the phen have EITHER more 1s or 0s\n  } # end test fisher\n\n\n  #######################\n  ## SIMULTANEOUS TEST ##\n  #######################\n  if(test == \"simultaneous\"){\n    corr.dat <- simultaneous.test(snps.reconstruction = snps, phen.reconstruction = phen, tree = tree)\n  } # end test simultaneous\n\n  #####################\n  ## SUBSEQUENT TEST ##\n  #####################\n  if(test == \"subsequent\"){\n    corr.dat <- subsequent.test(snps.reconstruction = snps, phen.reconstruction = phen, tree = tree)\n  } # end test subsequent\n\n\n\n  ## USE ABSOLUTE VALUE\n  if(test != \"subsequent\"){\n    corr.dat <- abs(corr.dat)\n  }\n\n  return(corr.dat)\n} # end assoc.test\n\n\n\n\n\n#################\n## .get.p.vals ##\n#################\n## NOTE: only used for FDR threshold calculation!\n## get a p-value associated with every value of corr.sim\n\n.get.p.vals <- function(corr.sim, corr.dat=NULL, fisher.test=FALSE){\n\n  p.vals <- NULL\n\n  ###################\n  ## CORR.SIM ONLY ##\n  ###################\n  if(is.null(corr.dat)){\n    ## faster with table:\n    cs.tab <- table(corr.sim)\n    cs.fac <- factor(corr.sim)\n\n    p.vals.unique <- sapply(c(1:(length(cs.tab)-1)),\n                            function(e)\n                              sum(cs.tab[(e+1):length(cs.tab)])\n                            /sum(cs.tab))\n    ## need to add trailing 0 for max corr.sim:\n    p.vals.unique <- c(p.vals.unique, 0)\n\n    ## get the unique index that\n    ## each original corr.sim should map to:\n    map.to <- (as.integer(cs.fac) - 1)\n    map.to <- map.to[!is.na(map.to)]\n    if(length(map.to)) map.to <- map.to + 1\n\n    ## get p.vals for all corr.sim:\n    p.vals <- p.vals.unique[map.to]\n  }else{\n\n    ###########################\n    ## CORR.DAT vs. CORR.SIM ##\n    ###########################\n    ## get table for corr.sim:\n    cs.tab <- table(corr.sim)\n\n    ## get table for corr.dat:\n    cd.tab <- table(corr.dat)\n    cd.fac <- factor(corr.dat)\n\n    ## get all possible p.vals | corr.sim:\n    p.vals.unique <- sapply(c(1:(length(cs.tab)-1)),\n                            function(e)\n                              sum(cs.tab[(e+1):length(cs.tab)])\n                            /sum(cs.tab))\n    ## need to add trailing 0 for max corr.sim:\n    p.vals.unique <- c(p.vals.unique, 0)\n\n    ## get real p.vals using corr.dat for break points:\n    p.vals.dat <- list()\n\n    if(fisher.test == FALSE){\n      for(i in 1:length(cd.tab)){\n        tab.above <- which(as.numeric(names(cs.tab)) > as.numeric(names(cd.tab[i])))\n        if(!.is.integer0(tab.above)){\n          p.vals.dat[[i]] <- sum(cs.tab[tab.above])/sum(cs.tab)\n        }else{\n          p.vals.dat[[i]] <- 0\n        }\n      }\n    }else{\n      ## fisher.test == TRUE --> Reverse sign (> --> <) ##\n      for(i in 1:length(cd.tab)){\n        tab.below <- which(as.numeric(names(cs.tab)) < as.numeric(names(cd.tab[i])))\n        if(!.is.integer0(tab.below)){\n          p.vals.dat[[i]] <- sum(cs.tab[tab.below])/sum(cs.tab)\n        }else{\n          p.vals.dat[[i]] <- 0\n        }\n      }\n    }\n    p.vals.dat <- as.vector(unlist(p.vals.dat))\n\n    ## get the unique index that\n    ## each original corr.dat should map to:\n    map.to <- (as.integer(cd.fac) - 1)\n    map.to <- map.to[!is.na(map.to)]\n    if(length(map.to)) map.to <- map.to + 1\n\n    ## get p.vals for all corr.dat:\n    p.vals <- p.vals.dat[map.to]\n  }\n\n  return(p.vals)\n} # end .get.p.vals\n",
    "created" : 1474604602380.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2131758885",
    "id" : "E9C2F614",
    "lastKnownWriteTime" : 1472488770,
    "last_content_update" : 1472488770,
    "path" : "D:/treeWAS/pkg/R/get.sig.snps.eval.R",
    "project_path" : "R/get.sig.snps.eval.R",
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}